# Dockerfile for kraken

FROM quay.io/krkn-chaos/krkn:latest

ENV KUBECONFIG /home/krkn/.kube/config

# Copy configurations
COPY config.yaml.template /home/krkn/kraken/config/config.yaml.template
COPY metrics_config.yaml.template /home/krkn/kraken/config/kube_burner.yaml.template
COPY power-outages/env.sh /home/krkn/env.sh
COPY env.sh /home/krkn/main_env.sh
COPY power-outages/run.sh /home/krkn/run.sh
COPY power-outages/shutdown_scenario.yaml.template /home/krkn/kraken/scenarios/shutdown_scenario.yaml.template
COPY power-outages/post_action.py /home/krkn/kraken/scenarios/post_action.py
COPY common_run.sh /home/krkn/common_run.sh

LABEL krknctl.kubeconfig_path="/home/krkn/.kube/config"
LABEL krknctl.title="Power outages"
LABEL krknctl.description="This scenario shuts down Kubernetes/OpenShift cluster for the specified duration \
to simulate power outages, brings it back online and checks if it's healthy. More information can be found \
here Right now power outage and cluster shutdown are one in the same. We originally created this scenario to \
stop all the nodes and then start them back up how a customer would shut their cluster down.In a real life \
chaos scenario though, we figured this scenario was close to if the power went out on the aws side so all of \
our ec2 nodes would be stopped/powered off. We tried to look at if aws cli had a way to forcefully poweroff \
the nodes (not gracefully) and they don't currently support so this scenario is as close as we can get to \"pulling the plug\""

LABEL krknctl.input_fields='$KRKNCTL_INPUT'

ENTRYPOINT /home/krkn/run.sh
